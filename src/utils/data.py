"""
Utilities for downloading learning dynamics checkpoint data from HuggingFace. 

NOTE: Assumes that models have been uploaded to HuggingFace via Pico.
"""

import os
import re
import torch
from huggingface_hub import snapshot_download, HfApi
from typing import Generator
from datasets import load_from_disk

def generate_learning_dynamics_data(run_path: str = None, repo_id: str = None, branch: str = None) -> Generator[tuple[int, dict], None, None]:
    """
    Returns a generator of all the available learning dynamics data available in a given run path 
    or a given HuggingFace repository and branch. We assume that the learning dynamics data is stored
    in the learning_dynamics folder in the checkpoint folder (generated by Pico) and has 
    the following structure:

    learning_dynamics/
        checkpoint/
            step_<step>/
                learning_dynamics/
                    train_activations.pt
                    train_weights.pt
                    train_gradients.pt
                    train_data.pt

    Args:
        run_path: Path to the run
        repo_id: HuggingFace repository ID
        branch: Branch to get commits from

    Returns:
        Generator[tuple[int, dict], None, None]: Generator of tuples containing (step, data_dict)
    """
    # check that we have either run_path or repo_id and branch
    if not run_path and not (repo_id and branch):
        raise ValueError("Must specify either run_path or repo_id and branch")

    if run_path:
        yield from _load_learning_dynamics_data(run_path)
    else:
        yield from _download_learning_dynamics_data(repo_id, branch)


####################
#
# Helper Functions for setting up learning dynamics data
#
####################

def _get_learning_dynamics_commits(repo_id: str, branch: str) -> dict:
    """
    Get the list of commits for a given repository and branch on HuggingFace.

    Args:
        repo_id: HuggingFace repository ID
        branch: Branch to get commits from

    Returns:
        dict: Dictionary containing the commits for train and val data.
    """

    api = HfApi()

    pattern = r"Saving Learning Dynamics Data \((train|val)\) -- Step (\d+)"

    # Create defaultdict to store commits by type and step
    learning_dynamics_commits_train = dict()
    learning_dynamics_commits_val = dict()

    # Get all commits
    commits = api.list_repo_commits(repo_id=repo_id, revision=branch)

    # Process each commit
    for commit in commits:
        # print(commit)
        match = re.search(pattern, commit.title)
        if match:
            data_type = match.group(1)  # 'train' or 'val'
            step = int(match.group(2))   # step number

            learning_dynamics_commits = learning_dynamics_commits_train if data_type == "train" else learning_dynamics_commits_val
            learning_dynamics_commits[step] = {
                'commit_id': commit.commit_id,
                'date': commit.created_at,
                'message': commit.title
            }

    return learning_dynamics_commits_train, learning_dynamics_commits_val

def _get_data_dict(learning_dynamics_path: str, data_split: list[str]) -> dict:
    """
    Given a learning dynamics path, load in the data and dataset.

    Args:
        learning_dynamics_path: Path to the learning dynamics

    Returns:
        dict: Dictionary containing the loaded learning dynamics data
    """
    # load the data
    data = {}
    for data_type in ['activations', 'weights', 'gradients']:
        file_path = os.path.join(learning_dynamics_path, f"{data_split}_{data_type}.pt")
        if os.path.exists(file_path):
            data[data_type] = torch.load(file_path)
    
    dataset_path = os.path.join(learning_dynamics_path, f"{data_split}_data")
    if os.path.exists(dataset_path):
        data['dataset'] = load_from_disk(dataset_path)
    
    return data

def _load_learning_dynamics_data(run_path: str) -> Generator[tuple[int, dict], None, None]:
    """
    Load learning dynamics data from a run path.

    Args:
        run_path: Path to the run

    Returns:
        Generator[tuple[int, dict], None, None]: Generator of tuples containing the step and the data
    """

    # ensure that the run_path is a valid path
    if not os.path.exists(run_path):
        raise ValueError(f"Run path {run_path} does not exist")

    checkpoint_path = os.path.join(run_path, "checkpoint")
    # ensure that the run_path contains a checkpoint folder
    if not os.path.exists(checkpoint_path):
        raise ValueError(f"Run path {run_path} does not contain a checkpoint folder")
    
    # get all of the steps in the checkpoint folder

    checkpoint_step_paths = [
        os.path.join(checkpoint_path, step_path) for step_path in os.listdir(checkpoint_path) if "step_" in step_path
    ]
    checkpoint_step_paths.sort()

    # iterate over the steps and yield the data
    for step_path in checkpoint_step_paths:
        step = int(step_path.split("step_")[-1])

        learning_dynamics_path = os.path.join(step_path, "learning_dynamics")
        learning_dynamics_data = {
            "train": _get_data_dict(learning_dynamics_path, "train"),
            "val": _get_data_dict(learning_dynamics_path, "val")
        }

        # get the data for the current step
        yield (step, learning_dynamics_data)

def _download_learning_dynamics_data(repo_id: str, branch: str) -> Generator[tuple[int, dict], None, None]:
    """
    Download learning dynamics data for a specific commit.
    
    Args:
        repo_id: HuggingFace repository ID
        branch: Branch to get commits from
        data_split: Either 'train' or 'val'
    
    Returns:
        dict: Dictionary containing the loaded learning dynamics data
    """

    # get all of the commits in the branch
    train_commits, val_commits = _get_learning_dynamics_commits(repo_id, branch)

    commit_steps = list(train_commits.keys())

    for step in commit_steps:
        train_commit = train_commits[step]
        val_commit = val_commits[step]

        # training data
        training_dir = snapshot_download(
            repo_id=repo_id,
            revision=train_commit['commit_id'],
        )

        training_learning_dynamics_path = os.path.join(training_dir, "learning_dynamics")
        training_data = _get_data_dict(training_learning_dynamics_path, "train")

        # validation data
        validation_dir = snapshot_download(
            repo_id=repo_id,
            revision=val_commit['commit_id'],
        )
        validation_learning_dynamics_path = os.path.join(validation_dir, "learning_dynamics")
        validation_data = _get_data_dict(validation_learning_dynamics_path, "val")

        learning_dynamics_data = {
            "train": training_data,
            "val": validation_data
        }

        yield (step, learning_dynamics_data)